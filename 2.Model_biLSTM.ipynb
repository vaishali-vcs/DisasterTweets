{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real or Not? NLP with Disaster Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In this notebook, we will be training a Neural Network and also Google's BERT Model on our dataset.\n",
    "Below is a list of  that we will try.\n",
    "1. Word Embeddings using Bag of Words\n",
    "2. Word Embeddings using TF IDF\n",
    "3. Word Embeddings using GloVe\n",
    "4. Showing Confusion Matrices on the validation set for the 2 trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries\n",
    "The below code intializes hyperparameters of the model and also import the necessary python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import (LSTM,\n",
    "                          Embedding,\n",
    "                          BatchNormalization,\n",
    "                          Dense,\n",
    "                          TimeDistributed,\n",
    "                          Dropout,\n",
    "                          Bidirectional,\n",
    "                          Flatten,\n",
    "                          GlobalMaxPool1D)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load data\n",
    "The below code loads the data and prints the shape of Train and Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7613 rows and 5 columns in train\n",
      "There are 3263 rows and 4 columns in test\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "submission = pd.read_csv('./sample_submission.csv')\n",
    "\n",
    "\n",
    "print('There are {} rows and {} columns in train'.format(dataset.shape[0],dataset.shape[1]))\n",
    "print('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))\n",
    "\n",
    "train = dataset.text.values\n",
    "test = test.text.values\n",
    "sentiments = dataset.target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. word embedding- transformation from words to vectors.¶\n",
    "The challenge with textual data is that it needs to be represented in a format that can be mathematically used in solving some problem. In simple words, we need to get an integer representation of a word.\n",
    "\n",
    "GloVe is an acronym for Global Vectors for Word Representation. This allows us to take a corpus of text, and intuitively transform each word in that corpus into a position in a high-dimensional space which means that similar words will be placed together.\n",
    "\n",
    "The first task is to download pre-trained word vectors that is available in 3 varieties : 50D, 100D and 200 Dimensional. We will try 100D here. Before we load the vectors in code, we have to understand how the text file is formatted.\n",
    "Each line of the text file contains a word, followed by N numbers. The N numbers describe the vector of the word’s position. N may vary depending on which vectors is downloaded, for us, N is 100, since we are using glove.6B.100d.\n",
    "\n",
    "Below is the summary of tasks if the below code:\n",
    "1. parse the GloVe vectors file and build the dictionary of it's vectors. \n",
    "2. text is split into words using word_tokenize to build the corpus.\n",
    "3. build the dictionary consisting of unique words from the corpus.\n",
    "4. create embedding vectors for each sample in Train with maximum length of 72 words.\n",
    "\n",
    "Notice the Train shape printed below. It has changed from 5 features to 72 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in the longest sentence in Train=72\n",
      "There are 7613 rows and 72 columns in train\n"
     ]
    }
   ],
   "source": [
    "def embed(corpus):\n",
    "        return word_tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(train)\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "longest_train = max(train, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "length_long_sentence = len(word_tokenize(longest_train))\n",
    "print(\"number of words in the longest sentence in Train={}\".format(length_long_sentence))\n",
    "\n",
    "\n",
    "padded_sentences = pad_sequences(embed(train), length_long_sentence, padding='post')\n",
    "test_sentences = pad_sequences( embed(test), length_long_sentence, padding='post')\n",
    "\n",
    "#Twitter Gloves\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "embedding_dim = 200\n",
    "glove_file = open('./glove-twitter/glove.twitter.27B.' + str(embedding_dim) + 'd.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "\n",
    "glove_file.close()\n",
    "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    if index >= vocab_length:\n",
    "        continue\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "    \n",
    "print('There are {} rows and {} columns in train'.format(padded_sentences.shape[0],padded_sentences.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Bidirectional LSTM Model with pre-trained GloVe word embeddings\n",
    "In the below section we will build 5 networks and train it using GloVe features as the inputs. The architecture of all the networks is same and below is it's pictorial representation.\n",
    "<img src=\"./images/blstm.png\" width=\"700\" height=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Model: 0\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 72, 200)           4540200   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 72, 144)           157248    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 144)               576       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 72)                10440     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 72)                5256      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 73        \n",
      "=================================================================\n",
      "Total params: 4,713,793\n",
      "Trainable params: 173,305\n",
      "Non-trainable params: 4,540,488\n",
      "_________________________________________________________________\n",
      "Train on 3806 samples, validate on 3807 samples\n",
      "Epoch 1/15\n",
      "3806/3806 [==============================] - 47s 12ms/step - loss: 0.7144 - acc: 0.6545 - val_loss: 0.5321 - val_acc: 0.7413\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53215, saving model to ./models/model_0.h5\n",
      "Epoch 2/15\n",
      "3806/3806 [==============================] - 37s 10ms/step - loss: 0.5483 - acc: 0.7441 - val_loss: 0.4669 - val_acc: 0.7815\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53215 to 0.46692, saving model to ./models/model_0.h5\n",
      "Epoch 3/15\n",
      "3806/3806 [==============================] - 37s 10ms/step - loss: 0.5036 - acc: 0.7809 - val_loss: 0.4714 - val_acc: 0.7867\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.46692\n",
      "Epoch 4/15\n",
      "3806/3806 [==============================] - 42s 11ms/step - loss: 0.4547 - acc: 0.8077 - val_loss: 0.4528 - val_acc: 0.8012\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.46692 to 0.45285, saving model to ./models/model_0.h5\n",
      "Epoch 5/15\n",
      "3806/3806 [==============================] - 38s 10ms/step - loss: 0.4426 - acc: 0.8140 - val_loss: 0.4325 - val_acc: 0.8114\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.45285 to 0.43245, saving model to ./models/model_0.h5\n",
      "Epoch 6/15\n",
      "3806/3806 [==============================] - 40s 10ms/step - loss: 0.4204 - acc: 0.8224 - val_loss: 0.4504 - val_acc: 0.8035\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.43245\n",
      "Epoch 7/15\n",
      "3806/3806 [==============================] - 39s 10ms/step - loss: 0.3940 - acc: 0.8405 - val_loss: 0.4455 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.43245\n",
      "Epoch 8/15\n",
      "3806/3806 [==============================] - 37s 10ms/step - loss: 0.3778 - acc: 0.8452 - val_loss: 0.4420 - val_acc: 0.8159\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43245\n",
      "Epoch 9/15\n",
      "3806/3806 [==============================] - 38s 10ms/step - loss: 0.3496 - acc: 0.8542 - val_loss: 0.4417 - val_acc: 0.8135\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.43245\n",
      "Epoch 10/15\n",
      "3806/3806 [==============================] - 42s 11ms/step - loss: 0.3388 - acc: 0.8715 - val_loss: 0.4716 - val_acc: 0.8054\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.43245\n",
      "Epoch 11/15\n",
      "3806/3806 [==============================] - 39s 10ms/step - loss: 0.3201 - acc: 0.8694 - val_loss: 0.4766 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.43245\n",
      "Epoch 12/15\n",
      "3806/3806 [==============================] - 41s 11ms/step - loss: 0.3054 - acc: 0.8783 - val_loss: 0.5108 - val_acc: 0.8022\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.43245\n",
      "Epoch 13/15\n",
      "3806/3806 [==============================] - 37s 10ms/step - loss: 0.2730 - acc: 0.8883 - val_loss: 0.5303 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.43245\n",
      "Epoch 14/15\n",
      "3806/3806 [==============================] - 38s 10ms/step - loss: 0.2927 - acc: 0.8841 - val_loss: 0.5351 - val_acc: 0.8022\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.43245\n",
      "Epoch 15/15\n",
      "3806/3806 [==============================] - 39s 10ms/step - loss: 0.2667 - acc: 0.8983 - val_loss: 0.5634 - val_acc: 0.8006\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.43245\n",
      "********************\n",
      "Model: 1\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 72, 200)           4540200   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 72, 144)           157248    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 144)               576       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 72)                10440     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 72)                5256      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 73        \n",
      "=================================================================\n",
      "Total params: 4,713,793\n",
      "Trainable params: 173,305\n",
      "Non-trainable params: 4,540,488\n",
      "_________________________________________________________________\n",
      "Train on 3806 samples, validate on 3807 samples\n",
      "Epoch 1/15\n",
      "3806/3806 [==============================] - 48s 13ms/step - loss: 0.7135 - acc: 0.6555 - val_loss: 0.4739 - val_acc: 0.7670\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.47385, saving model to ./models/model_1.h5\n",
      "Epoch 2/15\n",
      "3806/3806 [==============================] - 41s 11ms/step - loss: 0.5626 - acc: 0.7428 - val_loss: 0.4393 - val_acc: 0.7991\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.47385 to 0.43925, saving model to ./models/model_1.h5\n",
      "Epoch 3/15\n",
      "3806/3806 [==============================] - 50s 13ms/step - loss: 0.4895 - acc: 0.7835 - val_loss: 0.4302 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43925 to 0.43025, saving model to ./models/model_1.h5\n",
      "Epoch 4/15\n",
      "3806/3806 [==============================] - 39s 10ms/step - loss: 0.4591 - acc: 0.8022 - val_loss: 0.4603 - val_acc: 0.7922\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.43025\n",
      "Epoch 5/15\n",
      "3806/3806 [==============================] - 39s 10ms/step - loss: 0.4450 - acc: 0.8119 - val_loss: 0.4236 - val_acc: 0.8101\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.43025 to 0.42355, saving model to ./models/model_1.h5\n",
      "Epoch 6/15\n",
      "3806/3806 [==============================] - 39s 10ms/step - loss: 0.4233 - acc: 0.8203 - val_loss: 0.4250 - val_acc: 0.8054\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.42355\n",
      "Epoch 7/15\n",
      "3806/3806 [==============================] - 40s 11ms/step - loss: 0.3955 - acc: 0.8316 - val_loss: 0.4372 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.42355\n",
      "Epoch 8/15\n",
      "3806/3806 [==============================] - 38s 10ms/step - loss: 0.3723 - acc: 0.8481 - val_loss: 0.4375 - val_acc: 0.8135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: val_loss did not improve from 0.42355\n",
      "Epoch 9/15\n",
      "3806/3806 [==============================] - 38s 10ms/step - loss: 0.3642 - acc: 0.8513 - val_loss: 0.4496 - val_acc: 0.8174\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.42355\n",
      "Epoch 10/15\n",
      "3806/3806 [==============================] - 39s 10ms/step - loss: 0.3384 - acc: 0.8621 - val_loss: 0.4539 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.42355\n",
      "Epoch 11/15\n",
      "3806/3806 [==============================] - 42s 11ms/step - loss: 0.3100 - acc: 0.8805 - val_loss: 0.4735 - val_acc: 0.8077\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.42355\n",
      "Epoch 12/15\n",
      "3806/3806 [==============================] - 44s 11ms/step - loss: 0.3208 - acc: 0.8747 - val_loss: 0.4847 - val_acc: 0.8111\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.42355\n",
      "Epoch 13/15\n",
      "3806/3806 [==============================] - 44s 12ms/step - loss: 0.2788 - acc: 0.8904 - val_loss: 0.5139 - val_acc: 0.8114\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.42355\n",
      "Epoch 14/15\n",
      "3806/3806 [==============================] - 40s 11ms/step - loss: 0.2737 - acc: 0.8944 - val_loss: 0.5154 - val_acc: 0.8022\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.42355\n",
      "Epoch 15/15\n",
      "3806/3806 [==============================] - 38s 10ms/step - loss: 0.2549 - acc: 0.8933 - val_loss: 0.5594 - val_acc: 0.8138\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.42355\n",
      "********************\n",
      "Model: 2\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 72, 200)           4540200   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 72, 144)           157248    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 144)               576       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 72)                10440     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 72)                5256      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 73        \n",
      "=================================================================\n",
      "Total params: 4,713,793\n",
      "Trainable params: 173,305\n",
      "Non-trainable params: 4,540,488\n",
      "_________________________________________________________________\n",
      "Train on 3806 samples, validate on 3807 samples\n",
      "Epoch 1/15\n",
      "3806/3806 [==============================] - 49s 13ms/step - loss: 0.7505 - acc: 0.6203 - val_loss: 0.4989 - val_acc: 0.7573\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49890, saving model to ./models/model_2.h5\n",
      "Epoch 2/15\n",
      "3806/3806 [==============================] - 39s 10ms/step - loss: 0.5532 - acc: 0.7507 - val_loss: 0.4442 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49890 to 0.44421, saving model to ./models/model_2.h5\n",
      "Epoch 3/15\n",
      "3806/3806 [==============================] - 41s 11ms/step - loss: 0.4939 - acc: 0.7796 - val_loss: 0.4335 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.44421 to 0.43351, saving model to ./models/model_2.h5\n",
      "Epoch 4/15\n",
      "3806/3806 [==============================] - 40s 11ms/step - loss: 0.4585 - acc: 0.8069 - val_loss: 0.4515 - val_acc: 0.8014\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.43351\n",
      "Epoch 5/15\n",
      "3806/3806 [==============================] - 40s 10ms/step - loss: 0.4087 - acc: 0.8266 - val_loss: 0.4301 - val_acc: 0.8117\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.43351 to 0.43013, saving model to ./models/model_2.h5\n",
      "Epoch 6/15\n",
      "3806/3806 [==============================] - 40s 11ms/step - loss: 0.4044 - acc: 0.8295 - val_loss: 0.4435 - val_acc: 0.8122\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.43013\n",
      "Epoch 7/15\n",
      "3806/3806 [==============================] - 39s 10ms/step - loss: 0.3767 - acc: 0.8452 - val_loss: 0.4368 - val_acc: 0.8153\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.43013\n",
      "Epoch 8/15\n",
      "3806/3806 [==============================] - 39s 10ms/step - loss: 0.3612 - acc: 0.8526 - val_loss: 0.4429 - val_acc: 0.8146\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43013\n",
      "Epoch 9/15\n",
      "3806/3806 [==============================] - 39s 10ms/step - loss: 0.3453 - acc: 0.8571 - val_loss: 0.4867 - val_acc: 0.8148\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.43013\n",
      "Epoch 10/15\n",
      "3806/3806 [==============================] - 38s 10ms/step - loss: 0.3253 - acc: 0.8644 - val_loss: 0.5061 - val_acc: 0.8085\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.43013\n",
      "Epoch 11/15\n",
      "3806/3806 [==============================] - 38s 10ms/step - loss: 0.3106 - acc: 0.8765 - val_loss: 0.4917 - val_acc: 0.8132\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.43013\n",
      "Epoch 12/15\n",
      "3806/3806 [==============================] - 39s 10ms/step - loss: 0.2908 - acc: 0.8839 - val_loss: 0.5279 - val_acc: 0.8143\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.43013\n",
      "Epoch 13/15\n",
      "3806/3806 [==============================] - 42s 11ms/step - loss: 0.2851 - acc: 0.8836 - val_loss: 0.5397 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.43013\n",
      "Epoch 14/15\n",
      "3806/3806 [==============================] - 42s 11ms/step - loss: 0.2763 - acc: 0.8891 - val_loss: 0.5465 - val_acc: 0.8153\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.43013\n",
      "Epoch 15/15\n",
      "3806/3806 [==============================] - 40s 11ms/step - loss: 0.2438 - acc: 0.9046 - val_loss: 0.6058 - val_acc: 0.8098\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.43013\n",
      "********************\n",
      "Model: 3\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 72, 200)           4540200   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 72, 144)           157248    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 144)               576       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 72)                10440     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 72)                5256      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 73        \n",
      "=================================================================\n",
      "Total params: 4,713,793\n",
      "Trainable params: 173,305\n",
      "Non-trainable params: 4,540,488\n",
      "_________________________________________________________________\n",
      "Train on 3806 samples, validate on 3807 samples\n",
      "Epoch 1/15\n",
      "3806/3806 [==============================] - 52s 14ms/step - loss: 0.7494 - acc: 0.6350 - val_loss: 0.4911 - val_acc: 0.7704\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49112, saving model to ./models/model_3.h5\n",
      "Epoch 2/15\n",
      "3806/3806 [==============================] - 38s 10ms/step - loss: 0.5663 - acc: 0.7323 - val_loss: 0.4290 - val_acc: 0.8117\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49112 to 0.42901, saving model to ./models/model_3.h5\n",
      "Epoch 3/15\n",
      "3806/3806 [==============================] - 40s 10ms/step - loss: 0.5138 - acc: 0.7761 - val_loss: 0.4283 - val_acc: 0.8122\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.42901 to 0.42825, saving model to ./models/model_3.h5\n",
      "Epoch 4/15\n",
      "3806/3806 [==============================] - 39s 10ms/step - loss: 0.4807 - acc: 0.7937 - val_loss: 0.4209 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.42825 to 0.42087, saving model to ./models/model_3.h5\n",
      "Epoch 5/15\n",
      "3806/3806 [==============================] - 37s 10ms/step - loss: 0.4462 - acc: 0.8161 - val_loss: 0.4225 - val_acc: 0.8153\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.42087\n",
      "Epoch 6/15\n",
      "3806/3806 [==============================] - 37s 10ms/step - loss: 0.4400 - acc: 0.8095 - val_loss: 0.4154 - val_acc: 0.8190\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.42087 to 0.41535, saving model to ./models/model_3.h5\n",
      "Epoch 7/15\n",
      "3806/3806 [==============================] - 37s 10ms/step - loss: 0.4167 - acc: 0.8311 - val_loss: 0.4161 - val_acc: 0.8201\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.41535\n",
      "Epoch 8/15\n",
      "1248/3806 [========>.....................] - ETA: 21s - loss: 0.3962 - acc: 0.8405"
     ]
    }
   ],
   "source": [
    "def BLSTM():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                        output_dim=embedding_matrix.shape[1],\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=length_long_sentence,\n",
    "                        trainable=False))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(length_long_sentence, return_sequences=True, recurrent_dropout=0.2)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(length_long_sentence, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(length_long_sentence, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "for idx in range(5):\n",
    "    print(\"*\" * 20 + '\\nModel: ' + str(idx) + '\\n')\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau( monitor='val_loss', factor=0.2, verbose=1, patience=5, min_lr=0.001)\n",
    "\n",
    "    checkpoint = ModelCheckpoint('./models/model_' + str(idx) + '.h5',monitor='val_loss', mode='auto',verbose=1,\n",
    "                                 save_weights_only=True, save_best_only=True)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(padded_sentences, sentiments, test_size=0.5)\n",
    "\n",
    "    model = BLSTM()\n",
    "    model.fit(X_train, y_train, batch_size=32, epochs=15, validation_data=[X_test, y_test],\n",
    "                  callbacks=[reduce_lr, checkpoint], verbose=1)\n",
    "\n",
    "from glob import glob\n",
    "import scipy\n",
    "\n",
    "x_models = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now predict on test and create submission file that will be uploaded to kaggle. Here we are predicting classes using the 5 models generated and taking the mode for each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx in glob('*.h5'):\n",
    "    model = BLSTM()\n",
    "    model.load_weights(idx)\n",
    "    x_models.append(model)\n",
    "\n",
    "for idx in x_models:\n",
    "    preds = idx.predict_classes(test_sentences)\n",
    "    labels.append(preds)\n",
    "\n",
    "labels = scipy.stats.mode(labels)[0]\n",
    "labels = np.squeeze(labels)\n",
    "\n",
    "submission.target = labels\n",
    "submission.to_csv(\"glovetwitter.bilstmsample.submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
