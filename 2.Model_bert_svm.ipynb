{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook I try my first application of the BERT concept for NLP. As I understood I first have to clean up the data and then get it into a form that can be processed by BERT, so that the input sequences can be processed by a BERT layer that I have loaded from the tensorflow hub. I try to learn from the notebook https://www.kaggle.com/hassanamin/bert-nlp-real-or-not\n",
    "\n",
    "In this notebook we will prepare submission based on an ensemble of the predictions of the following four models:\n",
    "\n",
    "1. BERT pretrained word embeddings fed to a Dense Layer\n",
    "2. pretrained smaller dimensional word embeddings by Universal Sentence Encoder fed to SVM\n",
    "3. pretrained higher dimensional word embeddings by  Universal Sentence Encoder fed to a MultiLayerPerceptron\n",
    "4. pretrained higher dimensional word embeddings by  Universal Sentence Encoder fed to SVM\n",
    "\n",
    "# 2. Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub \n",
    "\n",
    "import keras\n",
    "from tensorflow.keras.layers import Dense, Input,LeakyReLU, Dropout, Softmax\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import bert\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the competition data\n",
    "train_df = pd.read_csv(\"./train.csv\")\n",
    "test_df = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "train_df = train_df.astype({\"id\" : int, \"target\" : int, \"text\" : str})\n",
    "test_df = test_df.astype({\"id\" : int, \"text\" : str})\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning\n",
    "\n",
    "To clean the freetext in Text column, we will be removing the contents:\n",
    "- Emojis\n",
    "- symbols & pictographs\n",
    "- hashtags\n",
    "- line breaks, leading, trailing, and extra spaces\n",
    "\n",
    "Also extracted hashtags, usernames and weblinks from text feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpful function for cleaning the text with regular experessions\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'https?://\\S+', '', text) # remove https? links\n",
    "    text = re.sub(r'#', '', text) # remove hashtags by keeping the hashtag text\n",
    "    text = re.sub(r'@\\w+', '', text) # remove @usernames\n",
    "    text = re.sub(r'\\n',' ', text) # remove line breaks\n",
    "    text = re.sub('\\s+', ' ', text).strip() # remove leading, trailing, and extra spaces\n",
    "    return text\n",
    "\n",
    "# helpful function for extract hashtags, usernames and weblinks from tweets\n",
    "def find_hashtags(tweet):\n",
    "    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or 'no'\n",
    "\n",
    "def find_usernames(tweet):\n",
    "    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"@\\w+\", tweet)]) or 'no'\n",
    "\n",
    "def find_links(tweet):\n",
    "    return \" \".join([match.group(0)[:] for match in re.finditer(r\"https?://\\S+\", tweet)]) or 'no'\n",
    "\n",
    "# function for pereprocessing the whole text\n",
    "def preprocess_text(df):\n",
    "    df['clean_text'] = df['text'].apply(lambda x: clean_text(x)) # cleaning the text\n",
    "    df['hashtags'] = df['text'].apply(lambda x: find_hashtags(x)) # extracting the hashtags\n",
    "    df['usernames'] = df['text'].apply(lambda x: find_usernames(x)) # extracting the @username(s)\n",
    "    df['links'] = df['text'].apply(lambda x: find_links(x)) # extracting http(s)-links\n",
    "    return df \n",
    "    \n",
    "# preprocessing the 'text'-column in df and extending with additional columns \n",
    "# 'clean_text', 'hashtags', 'usernames' and 'links'\n",
    "train_df = preprocess_text(train_df)\n",
    "test_df = preprocess_text(test_df)\n",
    "\n",
    "train_df.fillna(' ')\n",
    "test_df.fillna(' ')\n",
    "train_df['text_final'] = train_df['clean_text']+' '+ train_df['keyword']\n",
    "test_df['text_final'] = test_df['clean_text']+' '+ test_df['keyword']\n",
    "\n",
    "train_df['lowered_text'] = train_df['text_final'].str.lower()\n",
    "test_df['lowered_text'] = test_df['text_final'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Build and Train Models\n",
    "## 4.1. BERT Model\n",
    "\n",
    "In the field of computer vision, researchers have repeatedly shown the value of **transfer learning** — pre-training a neural network model on a known task, for instance ImageNet, and then performing fine-tuning i.e. using the trained neural network as the basis of a new purpose-specific model. In recent years, researchers have been showing that a similar technique can be useful in many natural language tasks. We will show how a pre-trained neural network produces word embeddings which are then used as features in NLP models.\n",
    "\n",
    "Now, we are trying a different model called BERT. BERT, which stands for **Bidirectional Encoder Representations from Transformers** and one of it's applications is text classification. BERT is a text representation technique like Word Embeddings. Like word embeddings, BERT is also a text representation technique which is a fusion of variety of state-of-the-art deep learning algorithms, such as bidirectional encoder LSTM and Transformers. BERT was developed by researchers **at Google AI Language in 2018** and has been proven to be state-of-the-art for a variety of natural language processing tasks such text classification, text summarization, text generation, etc. \n",
    "One of the mechanisms of the model is an **Transformer Encoder** that reads the text input. The input to Transformer Encoder is a sequence of tokens, which are first embedded into vectors and then processed in the neural network. The output is a sequence of vectors of size H, in which each vector corresponds to an input token with the same index.\n",
    "BERT can be used for a wide variety of language tasks, while only adding a small layer to the core model.\n",
    "Classification task is done by adding a \"classification layer\" on top of the Transformer output for the token.\n",
    "\n",
    "We are going to download the model using a url, where we can find all the prebuilt and pretrained models developed in TensorFlow. We will use the official tokenization script created by the Google team that is upload on github.  \n",
    "\n",
    "As a part of text cleaning we will be removing links and non-ASCII characters, emoji, punctuations and also convert abbreviations such as ppl, omg, fyi, etc.\n",
    "\n",
    "Following is the logic of the code in the next few cells-\n",
    "1. Load BERT model from the Tensorflow Hub(tfhub)\n",
    "2. Load tokenizer from the bert layer\n",
    "3. Encode the text into tokens, masks, and segment flags\n",
    "4. Modify the output layer of the pre-trained BERT model as follows and train-<br>\n",
    "  **input-text  ===> Encoding for bert ==> BERT  ===> Classifier(FeedForward-Network with 'softmax'-output-layer)** <br>Below is the pictorial representation of the architecture.\n",
    "  <img src=\"images/bert.png\" width=\"900\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding text for bert in an bert-compatible format like: [CLS]..text..[SEP][PAD][PAD] etc.\n",
    "#  cls_token='[CLS]', sep_token='[SEP]', pad_token='[PAD]'=[0], mask_token='[MASK]',\n",
    "# pass the text, the tokenizer from BERT an a max_len of the sequences \n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len):  # length of encoded sequences \n",
    "    # prepare empty np-arrays for the token, mask and segments\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:  # for every text-sequence\n",
    "        text = tokenizer.tokenize(text)# transform text-sequence into token-sequence\n",
    "          \n",
    "        text = text[:max_len-2]# cut the token-sequence at the end\n",
    "        \n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"] # insert [CLS]-token at the beginning of sequence and a [SEP]-token at the end\n",
    "        \n",
    "        pad_len = max_len - len(input_sequence) # determine the length of the [PAD]-sequences to add on short input-sequences\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) # transforms token to token-id\n",
    "        tokens += [0] * pad_len # concatenate the missing space as [0]-PAD-token\n",
    "       \n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len # pad_mask of the form 11111...00000 with 111 for input, 000 for rest\n",
    "        segment_ids = [0] * max_len # segment_id of the form 00000...000\n",
    "        \n",
    "        all_tokens.append(tokens) # concatenate the token-sequences\n",
    "        all_masks.append(pad_masks) # concatenate the padding-masks\n",
    "        all_segments.append(segment_ids) # concatenate the segment-ids\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments) # return all\n",
    "\n",
    "\n",
    "# define a model by pass a bert-layer and a finite sequence-lenght as parameters\n",
    "# to the function\n",
    "def build_model(bert_layer, max_len): # etc. max_len=512, bert encoder works with sequences of finite lenght\n",
    "    \n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :] # for sentence classification, we’re only  interested \n",
    "    #in BERT’s output for the [CLS] token, \n",
    "    \n",
    "    hidden1 = Dense(128, activation='relu')(clf_output) #128\n",
    "    out = Dense(1, activation='sigmoid')(hidden1) \n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 108310273   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 768)]        0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          98432       tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 108,408,834\n",
      "Trainable params: 108,408,833\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "\n",
    "# load a pretrained, trainable bert-layer as Keras.layer from the tensorflow-Hub\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1\",trainable=True)\n",
    "\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)\n",
    "\n",
    "# encoding the input- and test-features for BERT\n",
    "train_input = bert_encode(train_df.lowered_text.values.astype(str), tokenizer, max_len=512) # final input-data\n",
    "test_input = bert_encode(test_df.lowered_text.values.astype(str), tokenizer, max_len=512)# final test-data\n",
    "train_labels = train_df.target.values # final target-data\n",
    "\n",
    "model = build_model(bert_layer, max_len=512)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/5\n",
      "6090/6090 [==============================] - 421s 69ms/sample - loss: 0.5039 - accuracy: 0.7588 - val_loss: 0.4318 - val_accuracy: 0.7925\n",
      "Epoch 2/5\n",
      "6090/6090 [==============================] - 401s 66ms/sample - loss: 0.3751 - accuracy: 0.8401 - val_loss: 0.4073 - val_accuracy: 0.8043\n",
      "Epoch 3/5\n",
      "6090/6090 [==============================] - 401s 66ms/sample - loss: 0.3117 - accuracy: 0.8739 - val_loss: 0.4123 - val_accuracy: 0.8096\n",
      "Epoch 4/5\n",
      "6090/6090 [==============================] - 401s 66ms/sample - loss: 0.2503 - accuracy: 0.9051 - val_loss: 0.4305 - val_accuracy: 0.8102\n",
      "Epoch 5/5\n",
      "6090/6090 [==============================] - 401s 66ms/sample - loss: 0.1886 - accuracy: 0.9309 - val_loss: 0.4909 - val_accuracy: 0.8050\n",
      "[[0.6092735 ]\n",
      " [0.6092735 ]\n",
      " [0.6092735 ]\n",
      " [0.6092735 ]\n",
      " [0.6092735 ]\n",
      " [0.6092735 ]\n",
      " [0.6092735 ]\n",
      " [0.6092735 ]\n",
      " [0.6092735 ]\n",
      " [0.6092735 ]\n",
      " [0.6092735 ]\n",
      " [0.6092735 ]\n",
      " [0.6092735 ]\n",
      " [0.6092735 ]\n",
      " [0.6092735 ]\n",
      " [0.99568903]\n",
      " [0.07349035]\n",
      " [0.31909436]\n",
      " [0.01632243]\n",
      " [0.00715059]\n",
      " [0.01467803]\n",
      " [0.11484089]\n",
      " [0.03340209]\n",
      " [0.9929389 ]\n",
      " [0.0432989 ]\n",
      " [0.64377373]\n",
      " [0.01543269]\n",
      " [0.14688635]\n",
      " [0.08371693]\n",
      " [0.9987788 ]]\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=5, #5\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "predictions1 = model.predict(test_input)\n",
    "print(predictions1[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 SVC with embeddings from Universal-Sentence-Encoder\n",
    "\n",
    "While embedding a sentence, along with words the context of the whole sentence needs to be captured in that vector. This is where the “Universal Sentence Encoder” comes into the picture.\n",
    "If you recall the GloVe word embeddings vectors which turns a word to 50-dimensional vector, the Universal Sentence Encoder is much more powerful, and it is able to embed not only words but phrases and sentences. \n",
    "The **Universal Sentence Encoder (USE)** developed by researchers at **Google AI** encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering, and other natural language tasks. The pre-trained Universal Sentence Encoder is publicly available in **Tensorflow-hub**. It comes with two variations i.e. one trained with Transformer encoder and other trained with Deep Averaging Network (DAN). \n",
    "\n",
    "Following is the logic of the code in the next few cells-\n",
    "1. Load USE model from the Tensorflow Hub(tfhub)\n",
    "2. Create the embeddings for Train and Test\n",
    "4. Feed the embeddings to Support Vector Classifier(SVC) model and train-<br>\n",
    "  **input-text  ===> Embeddings from USE ==> SupportVectorClassifier** <br>Below is the pictorial representation of the architecture.\n",
    "  <img src=\"images/svm.png\" width=\"900\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try another model by using the google universal sentence encoder from https://tfhub.dev/google/universal-sentence-encoder/1 \n",
    "embedding = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "embedded_xtrain = embedding(train_df['clean_text']).numpy()\n",
    "embedded_xtest = embedding(test_df['clean_text']).numpy()\n",
    "target = train_df[\"target\"].to_numpy()\n",
    "\n",
    "# prepare a support vector maschine with radial basis funtion kernels\n",
    "from sklearn import svm\n",
    "model2 = svm.SVR(kernel='rbf',gamma='auto')\n",
    "model2.fit(emb edded_xtrain,target)\n",
    "\n",
    "predictions2 = model2.predict(embedded_xtest)\n",
    "predictions2 = np.mat(predictions2)\n",
    "predictions2 = predictions2.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 MultiLayerPerceptron with large Large dimensional embeddings from Universal-Sentence-Encoder\n",
    "\n",
    "Following is the logic of the 3rd model-\n",
    "1. Load USE model from the Tensorflow Hub(tfhub)\n",
    "2. Create the higher dimensional embeddings for Train and Test\n",
    "4. Feed the embeddings to a MultiLayerPercepton and train-<br>\n",
    "  <br>Below is the pictorial representation of the architecture.\n",
    "  <img src=\"images/mlp.png\" width=\"900\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try another model by using the google universal sentence encoder https://tfhub.dev/google/universal-sentence-encoder-lite/2\n",
    "sequence_lenght = 512\n",
    "USElite2_embedding = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\") #\"https://tfhub.dev/google/universal-sentence-encoder-lite/2\")\n",
    "\n",
    "USElite2_embedded_xtrain = USElite2_embedding(train_df['clean_text']).numpy()\n",
    "USElite2_embedded_xtest = USElite2_embedding(test_df['clean_text']).numpy()\n",
    "USElite2_target = train_df[\"target\"].to_numpy() # no embedding\n",
    "USElite2_embedded_xtest.shape\n",
    "\n",
    "USE_for_m4_xtrain = USElite2_embedded_xtrain\n",
    "USE_for_m4_xtest = USElite2_embedded_xtest\n",
    "USE_for_m4_target = USElite2_target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "USElite2_x_train, USElite2_x_test, USElite2_y_train, USElite2_y_test = train_test_split(\n",
    "    USElite2_embedded_xtrain,\n",
    "    USElite2_target,\n",
    "    test_size=0.1,\n",
    "    random_state=0,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_my_model():\n",
    "    input = keras.layers.Input(shape=(sequence_lenght,1), dtype='float32')\n",
    "    \n",
    "    #Conv1D-layer expected shape (Batchsize,Width,Channels)\n",
    "   \n",
    "    next_layer = keras.layers.Conv1D(265,kernel_size = 10, activation = \"relu\",padding=\"valid\",strides = 1)(input)\n",
    "    next_layer = keras.layers.MaxPooling1D(pool_size=2)(next_layer)\n",
    "    \n",
    "    next_layer = keras.layers.Conv1D(64,kernel_size = 5, padding=\"valid\", strides = 1)(next_layer)\n",
    "    next_layer = keras.layers.LeakyReLU(alpha=0.1)(next_layer)\n",
    "    next_layer = keras.layers.MaxPooling1D(pool_size=3, strides=1)(next_layer)\n",
    "    \n",
    "    next_layer = keras.layers.Flatten()(next_layer)\n",
    "    \n",
    "    next_layer = keras.layers.Dense(64)(next_layer)\n",
    "    next_layer = keras.layers.LeakyReLU(alpha=0.1)(next_layer)\n",
    "    \n",
    "    #next_layer = keras.layers.Dropout(0.2)(next_layer)\n",
    "    \n",
    "    #next_layer = keras.layers.LeakyReLU(alpha=0.1)(next_layer)\n",
    "    \n",
    "    output = keras.layers.Dense(1, activation=\"sigmoid\")(next_layer)\n",
    "      \n",
    "    return keras.Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 503, 265)          2915      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 251, 265)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 247, 64)           84864     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 247, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 245, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 15680)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                1003584   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,091,428\n",
      "Trainable params: 1,091,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6851 samples, validate on 762 samples\n",
      "Epoch 1/15\n",
      "6851/6851 [==============================] - 4s 571us/step - loss: 0.5684 - acc: 0.7025 - val_loss: 0.4629 - val_acc: 0.7861\n",
      "Epoch 2/15\n",
      "6851/6851 [==============================] - 1s 105us/step - loss: 0.4499 - acc: 0.8018 - val_loss: 0.4295 - val_acc: 0.8097\n",
      "Epoch 3/15\n",
      "6851/6851 [==============================] - 1s 109us/step - loss: 0.4077 - acc: 0.8244 - val_loss: 0.4200 - val_acc: 0.8150\n",
      "Epoch 4/15\n",
      "6851/6851 [==============================] - 1s 102us/step - loss: 0.3800 - acc: 0.8387 - val_loss: 0.4063 - val_acc: 0.8163\n",
      "Epoch 5/15\n",
      "6851/6851 [==============================] - 1s 102us/step - loss: 0.3592 - acc: 0.8492 - val_loss: 0.4301 - val_acc: 0.8110\n",
      "Epoch 6/15\n",
      "6851/6851 [==============================] - 1s 102us/step - loss: 0.3262 - acc: 0.8632 - val_loss: 0.4278 - val_acc: 0.8202\n",
      "Epoch 7/15\n",
      "6851/6851 [==============================] - 1s 101us/step - loss: 0.3033 - acc: 0.8759 - val_loss: 0.4406 - val_acc: 0.8110\n",
      "Epoch 8/15\n",
      "6851/6851 [==============================] - 1s 101us/step - loss: 0.2713 - acc: 0.8907 - val_loss: 0.4664 - val_acc: 0.8123\n",
      "Epoch 9/15\n",
      "6851/6851 [==============================] - 1s 102us/step - loss: 0.2403 - acc: 0.9066 - val_loss: 0.4921 - val_acc: 0.8123\n",
      "Epoch 10/15\n",
      "6851/6851 [==============================] - 1s 102us/step - loss: 0.2037 - acc: 0.9207 - val_loss: 0.5368 - val_acc: 0.8045\n",
      "Epoch 11/15\n",
      "6851/6851 [==============================] - 1s 103us/step - loss: 0.1783 - acc: 0.9337 - val_loss: 0.5804 - val_acc: 0.7730\n",
      "Epoch 12/15\n",
      "6851/6851 [==============================] - 1s 102us/step - loss: 0.1454 - acc: 0.9457 - val_loss: 0.6843 - val_acc: 0.7992\n",
      "Epoch 13/15\n",
      "6851/6851 [==============================] - 1s 100us/step - loss: 0.1362 - acc: 0.9510 - val_loss: 0.6538 - val_acc: 0.7992\n",
      "Epoch 14/15\n",
      "6851/6851 [==============================] - 1s 102us/step - loss: 0.1081 - acc: 0.9654 - val_loss: 0.6826 - val_acc: 0.8045\n",
      "Epoch 15/15\n",
      "6851/6851 [==============================] - 1s 101us/step - loss: 0.0934 - acc: 0.9693 - val_loss: 0.7708 - val_acc: 0.8097\n",
      "[[9.85813618e-01]\n",
      " [9.95270014e-01]\n",
      " [6.03817105e-01]\n",
      " [9.99999881e-01]\n",
      " [9.99999940e-01]\n",
      " [9.99534011e-01]\n",
      " [2.99840868e-02]\n",
      " [4.93631065e-02]\n",
      " [1.98666453e-02]\n",
      " [5.78549504e-03]\n",
      " [7.40889490e-01]\n",
      " [5.66989183e-04]\n",
      " [1.69411898e-02]\n",
      " [2.88972259e-03]\n",
      " [5.09545207e-03]\n",
      " [9.83061194e-01]\n",
      " [3.83479923e-01]\n",
      " [7.52777159e-02]\n",
      " [8.78036022e-04]\n",
      " [4.91937995e-03]\n",
      " [1.22599036e-01]\n",
      " [1.41336054e-01]\n",
      " [4.61590528e-01]\n",
      " [9.85420585e-01]\n",
      " [1.38647854e-02]\n",
      " [1.65498257e-03]\n",
      " [9.81116056e-01]\n",
      " [9.45050716e-02]\n",
      " [2.62382030e-02]\n",
      " [9.54202294e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the inputs. The conv1d-Layer needs (batchsize x lenght x dim=1)\n",
    "# shape[0]=batchsize=6090, shape[1]=length=512, dim=1\n",
    "USElite2_x_train = np.reshape(USElite2_x_train, (USElite2_x_train.shape[0], USElite2_x_train.shape[1],1))\n",
    "USElite2_y_train = np.reshape(USElite2_y_train, (USElite2_y_train.shape[0],1))\n",
    "# shape[0]=batchsize=1523, shape[1]=length=512, dim=1\n",
    "USElite2_x_test = np.reshape(USElite2_x_test, (USElite2_x_test.shape[0], USElite2_x_test.shape[1],1))\n",
    "USElite2_y_test = np.reshape(USElite2_y_test, (USElite2_y_test.shape[0],1))\n",
    "\n",
    "model3 = make_my_model()\n",
    "model3.compile(\"adam\", loss = \"binary_crossentropy\", metrics = [\"acc\"])\n",
    "model3.summary()\n",
    "\n",
    "model3.fit(\n",
    "    USElite2_x_train,\n",
    "    USElite2_y_train,\n",
    "    batch_size = 128,\n",
    "    epochs = 15,\n",
    "    validation_data = (USElite2_x_test,USElite2_y_test)\n",
    ")\n",
    "\n",
    "USElite2_embedded_xtest = np.reshape(USElite2_embedded_xtest, (USElite2_embedded_xtest.shape[0],USElite2_embedded_xtest.shape[1],1))\n",
    "predictions3 = model3.predict(USElite2_embedded_xtest)\n",
    "predictions3\n",
    "print(predictions3[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Support Vector Regression with Large Dimensional Embeddings from Universal Sentence Encoder\n",
    "\n",
    "Following is the logic of the 4th model-\n",
    "1. Load USE model from the Tensorflow Hub(tfhub)\n",
    "2. Create the higher dimensional embeddings for Train and Test\n",
    "4. Feed the embeddings to a MultiLayerPercepton and train-<br>\n",
    "  <br>Below is the pictorial representation of the architecture.\n",
    "  \n",
    "  <img src=\"images/svm.png\" width=\"900\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a support vector maschine with radial basis function kernels\n",
    "from sklearn import svm\n",
    "\n",
    "model4 = svm.SVR(kernel='rbf', gamma='auto')\n",
    "model4.fit(USE_for_m4_xtrain ,USE_for_m4_target)\n",
    "\n",
    "predictions4 = model4.predict(USE_for_m4_xtest)\n",
    "predictions4 = np.mat(predictions4)\n",
    "predictions4 = predictions4.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Voting of Predictions \n",
    "\n",
    "Below is the weights for predictions from the above 4 models:\n",
    "\n",
    "<br>\n",
    "(0.5 * Predictions from BERT model with Embeddings from BERT) + <br>\n",
    "(0.5 * Predictions from SVM with Lower Dimensional Embeddings from Universal Sentence Encoder) +  <br>\n",
    "(0.1 * Predictions from Perceptron with Large Dimensional Embeddings from Universal Sentence Encoder) +  <br>\n",
    "(0.3 * Predictions from SVM with Higher Dimensional Embeddings from Universal Sentence Encoder)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n",
    "\n",
    "submission['target'] =((0.5*predictions1+0.5*predictions2+0.1*predictions3+0.3*predictions4)*0.8).round().astype(int)\n",
    "\n",
    "submission.to_csv('submission_bert_svm_conv.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
